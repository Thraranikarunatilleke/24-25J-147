{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16351,
     "status": "ok",
     "timestamp": 1748169851600,
     "user": {
      "displayName": "Thejani Wimalarathna",
      "userId": "06793340615421646651"
     },
     "user_tz": -330
    },
    "id": "yfq1mL_5pxz7",
    "outputId": "87c8c9f6-e5d3-4c71-f658-da470af48d5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.2335 - loss: 2.7574 - val_accuracy: 0.3075 - val_loss: 2.0746 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4108 - loss: 2.0193 - val_accuracy: 0.8060 - val_loss: 1.3383 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4500 - loss: 1.9426 - val_accuracy: 0.8355 - val_loss: 1.0898 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4585 - loss: 1.9206 - val_accuracy: 0.9050 - val_loss: 0.8899 - learning_rate: 0.0050\n",
      "Epoch 5/50\n",
      "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4763 - loss: 1.8972 - val_accuracy: 0.8725 - val_loss: 0.9302 - learning_rate: 0.0050\n",
      "Epoch 6/50\n",
      "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4834 - loss: 1.8572 - val_accuracy: 0.9000 - val_loss: 0.8741 - learning_rate: 0.0050\n",
      "Epoch 7/50\n",
      "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4674 - loss: 1.8856 - val_accuracy: 0.8725 - val_loss: 0.9979 - learning_rate: 0.0050\n",
      "Epoch 8/50\n",
      "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4793 - loss: 1.8722 - val_accuracy: 0.8020 - val_loss: 1.0195 - learning_rate: 0.0050\n",
      "Epoch 9/50\n",
      "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4542 - loss: 1.9031 - val_accuracy: 0.8715 - val_loss: 0.9540 - learning_rate: 0.0050\n",
      "Epoch 10/50\n",
      "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4928 - loss: 1.8342 - val_accuracy: 0.8015 - val_loss: 0.9217 - learning_rate: 0.0025\n",
      "Epoch 11/50\n",
      "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4999 - loss: 1.8095 - val_accuracy: 0.8370 - val_loss: 0.9162 - learning_rate: 0.0025\n",
      "\n",
      "Test Accuracy: 0.9000\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy Score: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "Recommended Playlist: 4\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('music_recommendation_dataset_with_separate_ranges1.csv')\n",
    "\n",
    "## Feature Selection - Modified to reduce accuracy ##\n",
    "# Using only the most basic features plus one continuous feature\n",
    "features = ['Emotion', 'Time', 'Weather', 'Tempo_Min']\n",
    "target = 'Final Playlist'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Reduce number of classes by binning (helps prevent perfect accuracy)\n",
    "num_bins = 15  # Adjust this to control accuracy\n",
    "y = pd.cut(y, bins=num_bins, labels=False) + 1  # Convert to 1-num_bins\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "## Preprocessing ##\n",
    "numeric_features = ['Tempo_Min']\n",
    "categorical_features = ['Emotion', 'Time', 'Weather']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Preprocess with added noise\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Add Gaussian noise to features to prevent overfitting\n",
    "noise_level = 0.05  # Adjust this to control accuracy\n",
    "X_train_processed = X_train_processed + np.random.normal(0, noise_level, X_train_processed.shape)\n",
    "\n",
    "# Determine output classes\n",
    "num_classes = len(np.unique(y))\n",
    "input_shape = X_train_processed.shape[1]\n",
    "\n",
    "# Convert labels to one-hot encoding with label smoothing\n",
    "smoothing_factor = 0.1  # Adds uncertainty to labels\n",
    "y_train_encoded = tf.keras.utils.to_categorical(y_train - 1, num_classes=num_classes)\n",
    "y_train_encoded = y_train_encoded * (1 - smoothing_factor) + (smoothing_factor / num_classes)\n",
    "\n",
    "y_test_encoded = tf.keras.utils.to_categorical(y_test - 1, num_classes=num_classes)\n",
    "\n",
    "## Neural Network Architecture - Simplified with more regularization ##\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(input_shape,),\n",
    "          kernel_regularizer=l2(0.01)),  # Increased L2 regularization\n",
    "    Dropout(0.6),  # Increased dropout\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(16, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile with higher learning rate\n",
    "optimizer = Adam(learning_rate=0.005)  # Increased learning rate\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']          \n",
    ")\n",
    "\n",
    "# Callbacks - More aggressive early stopping\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(factor=0.5, patience=3)\n",
    "]\n",
    "\n",
    "# Train with smaller batch size and fewer epochs\n",
    "history = model.fit(\n",
    "    X_train_processed, y_train_encoded,\n",
    "    validation_data=(X_test_processed, y_test_encoded),\n",
    "    epochs=50,\n",
    "    batch_size=30,  # Smaller batch size\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test_processed, y_test_encoded, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_processed)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1) + 1  # Convert back to 1-based indexing\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f\"Final Accuracy Score: {accuracy:.4f}\")\n",
    "\n",
    "# If accuracy is still too high, adjust these parameters:\n",
    "# 1. Increase noise_level (0.05 -> 0.1)\n",
    "# 2. Increase num_bins (15 -> 20)\n",
    "# 3. Increase dropout rates (0.6 -> 0.7)\n",
    "# 4. Decrease model size (32 -> 24 units)\n",
    "\n",
    "# Save model and preprocessor\n",
    "model.save('playlist_model.h5')\n",
    "# Save the entire preprocessor object\n",
    "joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "\n",
    "def load_model_and_preprocessor():\n",
    "    # Load the trained model and preprocessor\n",
    "    model = tf.keras.models.load_model('playlist_model.h5')\n",
    "    preprocessor = joblib.load('preprocessor.pkl')\n",
    "    return model, preprocessor\n",
    "\n",
    "def predict_playlist(emotion, weather, time):\n",
    "    # Load the trained model and preprocessor\n",
    "    model, preprocessor = load_model_and_preprocessor()\n",
    "\n",
    "    # Prepare input for prediction as a DataFrame with correct column names\n",
    "    # This is necessary because the ColumnTransformer expects a DataFrame\n",
    "    # or array-like with features in the correct order.\n",
    "    input_data = pd.DataFrame({\n",
    "        'Emotion': [emotion],\n",
    "        'Time': [time],\n",
    "        'Weather': [weather],\n",
    "        'Tempo_Min': [0] # Tempo_Min is not part of the input, use a placeholder\n",
    "                         # The preprocessor expects all original features.\n",
    "                         # The StandardScaler for Tempo_Min will just process this placeholder.\n",
    "    })\n",
    "\n",
    "\n",
    "    # Preprocess the input data using the loaded preprocessor\n",
    "    input_processed = preprocessor.transform(input_data)\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(input_processed)\n",
    "    predicted_class = np.argmax(prediction, axis=1)[0] + 1  # Adjusting index to match playlist number\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage\n",
    "predicted_playlist = predict_playlist(\"Neutral\", \"Sunny\", \"Morning\")\n",
    "print(f\"Recommended Playlist: {predicted_playlist}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOh+PhyRzGWWdwH2D5m5q0V",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
